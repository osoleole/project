{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astra/anaconda3/envs/lamedoc/lib/python3.5/site-packages/gensim/utils.py:1015: UserWarning: Pattern library is not installed, lemmatization won't be available.\n",
      "  warnings.warn(\"Pattern library is not installed, lemmatization won't be available.\")\n"
     ]
    }
   ],
   "source": [
    "#Run this cell once \n",
    "\n",
    "import sys\n",
    "import os\n",
    "import codecs\n",
    "import spacy\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "PROJECT_ROOT = '/home/astra/work/projects/lamedoc_new/project/'\n",
    "if os.path.dirname(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(os.path.dirname(PROJECT_ROOT))\n",
    "\n",
    "from src.utils.files_paths import init_paths\n",
    "\n",
    "# Load language model. Takes time.\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Helper functions for processing\n",
    "def is_punct(token):\n",
    "    \"\"\"\n",
    "    Returns True if token is punctuation or space\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_generator(filename):\n",
    "    \"\"\"\n",
    "    Returns escaped line generator\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for line in f:\n",
    "            yield line.replace('\\\\n', '\\n')\n",
    "\n",
    "def text_lemmatizer(filename):\n",
    "    \"\"\"\n",
    "    Lemmatizes the text, and yield sentences\n",
    "    \"\"\"\n",
    "    for parsed_chunk in nlp.pipe(line_generator(filename),batch_size=10000, n_threads=4):\n",
    "        for sent in parsed_chunk.sents:\n",
    "            yield ' '.join([token.lemma_ for token in sent if not is_punct(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define raw text file\n",
    "from settings import settings\n",
    "RAW_TEXT_FILE_NAME = 'sec_wiki_books_requests.txt'\n",
    "raw_text_file = os.path.join(settings.RAW_DATA_PATH, RAW_TEXT_FILE_NAME)\n",
    "\n",
    "# Define ngrams paths\n",
    "unigram_text_filepath = init_paths(RAW_TEXT_FILE_NAME)['unigram_text_filepath']\n",
    "bigram_model_filepath = init_paths(RAW_TEXT_FILE_NAME)['bigram_model_filepath']\n",
    "bigram_text_filepath = init_paths(RAW_TEXT_FILE_NAME)['bigram_text_filepath']\n",
    "trigram_model_filepath = init_paths(RAW_TEXT_FILE_NAME)['trigram_model_filepath']\n",
    "trigram_text_filepath = init_paths(RAW_TEXT_FILE_NAME)['trigram_text_filepath']\n",
    "normalized_text_filepath = init_paths(RAW_TEXT_FILE_NAME)['normalized_text_filepath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loads RAW_TEXT_FILE_NAME and save as unigram text.\n",
    "if True:\n",
    "    with codecs.open(unigram_text_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in text_lemmatizer(raw_text_file):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_text = LineSentence(unigram_text_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating bigram model\n",
    "if True:\n",
    "    bigram_model = Phrases(unigram_text)\n",
    "    bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser\n",
    "# Load bigram model\n",
    "bigram_model = Phraser.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astra/anaconda3/envs/lamedoc/lib/python3.5/site-packages/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Making bigram text.\n",
    "if True:\n",
    "    with codecs.open(bigram_text_filepath, 'w', encoding='utf_8') as f:\n",
    "        for unigram_sentence in unigram_text:\n",
    "            bigram_sentence = ' '.join(bigram_model[unigram_sentence])\n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_text = LineSentence(bigram_text_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating trigram.\n",
    "if True:\n",
    "    trigram_model = Phrases(bigram_text)\n",
    "    trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load trigram model\n",
    "trigram_model = Phraser.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astra/anaconda3/envs/lamedoc/lib/python3.5/site-packages/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Generating trigram text\n",
    "if True:\n",
    "    with codecs.open(trigram_text_filepath, 'w', encoding='utf_8') as f:\n",
    "        for bigram_sentence in bigram_text:\n",
    "            trigram_sentence = ' '.join(trigram_model[bigram_sentence])\n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_text = LineSentence(trigram_text_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astra/anaconda3/envs/lamedoc/lib/python3.5/site-packages/gensim/models/phrases.py:248: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "# Generating normalized text\n",
    "if True:\n",
    "    with codecs.open(normalized_text_filepath, 'w', encoding='utf_8') as f:\n",
    "        for parsed_text in nlp.pipe(line_generator(raw_text_file),\n",
    "                                      batch_size=10000, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_text = [token.lemma_ for token in parsed_text\n",
    "                              if not is_punct(token)]\n",
    "            #print(\"unigram\")\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_text = bigram_model[unigram_text]\n",
    "            trigram_text = trigram_model[bigram_text]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_text = [term for term in trigram_text\n",
    "                              if term not in spacy.en.language_data.STOP_WORDS]\n",
    "            #print(\"trigram\")\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_text = ' '.join(trigram_text)\n",
    "            f.write(trigram_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:lamedoc]",
   "language": "python",
   "name": "conda-env-lamedoc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
