{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NLP procedures \n",
    "\n",
    "This procedures includes:\n",
    "1. Text files parsing.\n",
    "2. Text files cleaning.\n",
    "3. Text lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load spacy. Takes time. Spacy yet has a model of english languge. \n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define raw text file\n",
    "RAW_TEXT_FILE_NAME = 'all_clean_one_line.txt'\n",
    "raw_text_file = os.path.join(settings.RAW_DATA_PATH, RAW_TEXT_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load raw text file\n",
    "with codecs.open(raw_text_file, 'r', encoding='utf-8') as f:\n",
    "    sample_test = list(it.islice(f,0,500))[0:500]\n",
    "    sample_test = ' '.join(sample_test)\n",
    "    sample_test = re.sub('[\\n \\t]+',' ', sample_test)\n",
    "    sample_test = re.sub('\\x0c2', '', sample_test)\n",
    "\n",
    "#print(sample_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parse text with spacy nlp\n",
    "\n",
    "parsed_text = nlp(sample_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Detect sentences and enumerate them\n",
    "if False:\n",
    "    for num, sentence in enumerate(parsed_text.sents):\n",
    "        print('<Sentence {}>:'.format(num + 1))\n",
    "        print(sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Entity detection\n",
    "if False:\n",
    "    for num, entity in enumerate(parsed_text.ents):\n",
    "        print('<Entity {}>:'.format(num + 1), entity, '-', entity.label_)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# POS tagging\n",
    "token_text = [token.orth_ for token in parsed_text]\n",
    "token_pos = [token.pos_ for token in parsed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to data frame\n",
    "pd.DataFrame(list(zip(token_text, token_pos)), columns=['token', 'pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalization lemmatization\n",
    "token_lemma = [token.lemma_ for token in parsed_text]\n",
    "token_shape = [token.shape_ for token in parsed_text]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_lemma, token_shape)), columns=['text', 'lemma', 'shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Token levels analysis\n",
    "token_entity_type = [token.ent_type_ for token in parsed_text]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_text]\n",
    "\n",
    "pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "             columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Common statistics\n",
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in parsed_text]\n",
    "\n",
    "# Convert to panda's DataFrame\n",
    "attributes_df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "attributes_df.loc[:, 'stop?':'out of vocab.?'] = (attributes_df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: 'Yes' if x else ''))\n",
    "\n",
    "# Show attributes\n",
    "attributes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level text processing\n",
    "Colocations and phrases detection\n",
    "It is very important to subdivide text to sentences. Colocations and phrases do not cross the borders of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "#import settings \n",
    "\n",
    "unigram_text_filepath = os.path.join(settings.NGRAMS_DATA_PATH, 'unigram_text.txt')\n",
    "bigram_model_filepath = os.path.join(settings.NGRAMS_DATA_PATH, 'bigram_model')\n",
    "bigram_text_filepath = os.path.join(settings.NGRAMS_DATA_PATH, 'bigram_text.txt')\n",
    "trigram_model_filepath = os.path.join(settings.NGRAMS_DATA_PATH, 'trigram_model')\n",
    "trigram_text_filepath = os.path.join(settings.NGRAMS_DATA_PATH, 'trigram_text.txt')\n",
    "\n",
    "normalized_text_filepath = os.path.join(settings.NORMALIZED_DATA_PATH, 'normalized_text.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for processing\n",
    "def is_punct(token):\n",
    "    \"\"\"\n",
    "    Returns True if token is punctuation or space\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_generator(filename):\n",
    "    \"\"\"\n",
    "    Returns escaped line generator\n",
    "    \"\"\"\n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "\n",
    "def text_lemmatizer(filename):\n",
    "    \"\"\"\n",
    "    Lemmatizes the text, and yield sentences\n",
    "    \"\"\"\n",
    "    for parsed_chunk in nlp.pipe(line_generator(filename),batch_size=10000, n_threads=4):\n",
    "        for sent in parsed_chunk.sents:\n",
    "            yield ' '.join([token.lemma_ for token in sent if not is_punct(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loads RAW_TEXT_FILE_NAME and save as unigram text.\n",
    "if True:\n",
    "    with codecs.open(unigram_text_filepath, 'w', encoding='utf_8') as f:\n",
    "        for sentence in text_lemmatizer(raw_text_file):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_text = LineSentence(unigram_text_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating bigram model\n",
    "if True:\n",
    "    bigram_model = Phrases(unigram_text)\n",
    "    bigram_model.save(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load bigram model\n",
    "bigram_model = Phrases.load(bigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making bigram text.\n",
    "if True:\n",
    "    with codecs.open(bigram_text_filepath, 'w', encoding='utf_8') as f:\n",
    "        for unigram_sentence in unigram_text:\n",
    "            bigram_sentence = ' '.join(bigram_model[unigram_sentence])\n",
    "            f.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_text = LineSentence(bigram_text_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generating trigram.\n",
    "if True:\n",
    "    trigram_model = Phrases(bigram_text)\n",
    "    trigram_model.save(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load trigram model\n",
    "trigram_model = Phrases.load(trigram_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating trigram text\n",
    "if True:\n",
    "    with codecs.open(trigram_text_filepath, 'w', encoding='utf_8') as f:\n",
    "        for bigram_sentence in bigram_text:\n",
    "            trigram_sentence = u' '.join(trigram_model[bigram_sentence])\n",
    "            f.write(trigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trigram_text = LineSentence(trigram_text_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generating normalized text\n",
    "if True:\n",
    "    with codecs.open(normalized_text_filepath, 'w', encoding='utf_8') as f:\n",
    "        for parsed_text in nlp.pipe(line_generator(raw_text_file),\n",
    "                                      batch_size=20000, n_threads=4):\n",
    "            \n",
    "            # lemmatize the text, removing punctuation and whitespace\n",
    "            unigram_text = [token.lemma_ for token in parsed_text\n",
    "                              if not is_punct(token)]\n",
    "            print(\"unigram\")\n",
    "            \n",
    "            # apply the first-order and second-order phrase models\n",
    "            bigram_text = bigram_model[unigram_text]\n",
    "            trigram_text = trigram_model[bigram_text]\n",
    "            \n",
    "            # remove any remaining stopwords\n",
    "            trigram_text = [term for term in trigram_text\n",
    "                              if term not in spacy.en.STOPWORDS]\n",
    "            print(\"trigram\")\n",
    "            \n",
    "            # write the transformed review as a line in the new file\n",
    "            trigram_text = ' '.join(trigram_text)\n",
    "            f.write(trigram_text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:lamedoc]",
   "language": "python",
   "name": "conda-env-lamedoc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
